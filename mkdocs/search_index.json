{
    "docs": [
        {
            "location": "/",
            "text": "Welcome to Parallel ML Docs\n\n\nThis organization is from Georgia Tech, \nHPArch\n.",
            "title": "Home"
        },
        {
            "location": "/#welcome-to-parallel-ml-docs",
            "text": "This organization is from Georgia Tech,  HPArch .",
            "title": "Welcome to Parallel ML Docs"
        },
        {
            "location": "/asplos2018/real-time/",
            "text": "Introduction\n\n\nThis is demo of \nReal-Time Image Recognition Using Collaborative IoT Devices\n at \nACM ReQuEST workshop co-located with ASPLOS 2018\n\n\nGithub repo\n\n\nThis repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art \ndeep learning neural networks, AlexNet[2] and VGG16[3].\n\n\nInstallation\n\n\nPlease make sure that you have \nPython 2.7\n running on your device. We have\ntwo versions of model inference. One is using GPU and running model inference on\nsingle machine. Another is using CPU and using RPC to off-shore the computation\nto other devices. We will have different installation guide for those two versions\nmodel inference. \n\n\nSingle device (GPU and CPU).\n\n\n(This is NVidia Jetson TX2 version in our paper)\n\n\nDependencies:\n\n tensorflow-gpu >= 1.5.0\n\n Keras >= 2.1.3\n\n\npip install keras\n\n\n\n\nPlease refer to official installation guideline from Keras for more information\n\n\nMultiple devices (CPU and RPC).\n\n\n(This is Raspberry PI 3 versions in our paper)\n\n\nDependencies:\n\n tensorflow >= 1.5.0\n\n Keras >= 2.1.3\n* avro >= 1.8.2\n\n\nWe have provided dependency file here. You can execute this file to install packages.\n\n\npip install -r requirements.txt\n\n\n\n\nQuick Start\n\n\nSingle device (GPU and CPU)\n\n\n(This is NVidia Jetson TX2 version in our paper)\n\n\nGPU Version\n\n\nExecute predict file to run model inference. \n\n\npython predict.py\n\n\n\n\nCPU Version\n\n\nCUDA_VISIBLE_DEVICES= python predict.py\n\n\n\n\nMultiple devices (CPU and RPC)\n\n\n(This is Raspberry PI 3 versions in our paper)\n\n\nWe make a checklist for you before running our program.\n- [ ] Have all correct packages installed on Raspberry Pi. \n- [ ] The Raspberry PI has port 12345, 9999 open. \n- [ ] Put correct IP address in IP table file \nmutiple-devices/alexnet/resource/ip\n. \nThe IP table file is in \njson\n format. \n\n\nAlexNet\n\n\nFor AlexNet, we have same model partition, so we will use the same node file for \ndifferent system setup. The IP table is default to 4 devices setup. You need to \nadd 1 more IP address to \nblock1\n if you want to test 6 devices setup.\n\n\n\n\n\n\nOn all of your device except the initial sender, run the node.\n\n\n\n\npython node.py\n\n\n\n\n\n\nStart the data sender. You should be able to see console log.\n\n\n\n\npython initial.py\n\n\n\n\n\n\nIf you modify our code, you can use flag to debug.\n\n\n\n\npython node.py -d\n\n\n\n\nVGG16\n\n\nFor VGG16, we have different model separation for different system setup, so we put\ntwo directories under \nmutiple-devices/vgg16\n. For \n8devices\n, you should have 3 devices for\n\nblock234\n and 2 devices for \nfc1\n, which means you need 2 IP addresses for those\n2 blocks in IP table. For \n11devices\n, you should have 7 devices for \nblock12345\n,\nso put 7 IP addresses at IP table. \n\n\n\n\n\n\nOn all of your device except the initial sender, run the node.\n\n\n\n\npython node.py\n\n\n\n\n\n\nStart the data sender. You should be able to see console log.\n\n\n\n\npython initial.py\n\n\n\n\nRefereces\n\n\n[1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138.\n\n\n[2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012.\n\n\n[3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.",
            "title": "Real time"
        },
        {
            "location": "/asplos2018/real-time/#introduction",
            "text": "This is demo of  Real-Time Image Recognition Using Collaborative IoT Devices  at  ACM ReQuEST workshop co-located with ASPLOS 2018  Github repo  This repository contains demo files for demonstration of Musical Chair[1] applied on two state-of-art \ndeep learning neural networks, AlexNet[2] and VGG16[3].",
            "title": "Introduction"
        },
        {
            "location": "/asplos2018/real-time/#installation",
            "text": "Please make sure that you have  Python 2.7  running on your device. We have\ntwo versions of model inference. One is using GPU and running model inference on\nsingle machine. Another is using CPU and using RPC to off-shore the computation\nto other devices. We will have different installation guide for those two versions\nmodel inference.",
            "title": "Installation"
        },
        {
            "location": "/asplos2018/real-time/#single-device-gpu-and-cpu",
            "text": "(This is NVidia Jetson TX2 version in our paper)  Dependencies:  tensorflow-gpu >= 1.5.0  Keras >= 2.1.3  pip install keras  Please refer to official installation guideline from Keras for more information",
            "title": "Single device (GPU and CPU)."
        },
        {
            "location": "/asplos2018/real-time/#multiple-devices-cpu-and-rpc",
            "text": "(This is Raspberry PI 3 versions in our paper)  Dependencies:  tensorflow >= 1.5.0  Keras >= 2.1.3\n* avro >= 1.8.2  We have provided dependency file here. You can execute this file to install packages.  pip install -r requirements.txt",
            "title": "Multiple devices (CPU and RPC)."
        },
        {
            "location": "/asplos2018/real-time/#quick-start",
            "text": "",
            "title": "Quick Start"
        },
        {
            "location": "/asplos2018/real-time/#single-device-gpu-and-cpu_1",
            "text": "(This is NVidia Jetson TX2 version in our paper)",
            "title": "Single device (GPU and CPU)"
        },
        {
            "location": "/asplos2018/real-time/#gpu-version",
            "text": "Execute predict file to run model inference.   python predict.py",
            "title": "GPU Version"
        },
        {
            "location": "/asplos2018/real-time/#cpu-version",
            "text": "CUDA_VISIBLE_DEVICES= python predict.py",
            "title": "CPU Version"
        },
        {
            "location": "/asplos2018/real-time/#multiple-devices-cpu-and-rpc_1",
            "text": "(This is Raspberry PI 3 versions in our paper)  We make a checklist for you before running our program.\n- [ ] Have all correct packages installed on Raspberry Pi. \n- [ ] The Raspberry PI has port 12345, 9999 open. \n- [ ] Put correct IP address in IP table file  mutiple-devices/alexnet/resource/ip . \nThe IP table file is in  json  format.",
            "title": "Multiple devices (CPU and RPC)"
        },
        {
            "location": "/asplos2018/real-time/#alexnet",
            "text": "For AlexNet, we have same model partition, so we will use the same node file for \ndifferent system setup. The IP table is default to 4 devices setup. You need to \nadd 1 more IP address to  block1  if you want to test 6 devices setup.    On all of your device except the initial sender, run the node.   python node.py   Start the data sender. You should be able to see console log.   python initial.py   If you modify our code, you can use flag to debug.   python node.py -d",
            "title": "AlexNet"
        },
        {
            "location": "/asplos2018/real-time/#vgg16",
            "text": "For VGG16, we have different model separation for different system setup, so we put\ntwo directories under  mutiple-devices/vgg16 . For  8devices , you should have 3 devices for block234  and 2 devices for  fc1 , which means you need 2 IP addresses for those\n2 blocks in IP table. For  11devices , you should have 7 devices for  block12345 ,\nso put 7 IP addresses at IP table.     On all of your device except the initial sender, run the node.   python node.py   Start the data sender. You should be able to see console log.   python initial.py",
            "title": "VGG16"
        },
        {
            "location": "/asplos2018/real-time/#refereces",
            "text": "[1]: R. Hadidi, J. Cao, M. Woodward, M. Ryoo, and H. Kim, \"Musical Chair: Efficient Real-Time Recognition Using Collaborative IoT Devices,\" ArXiv e-prints:1802.02138.  [2]: A. Krizhevsky, I. Sutskever, and G. E. Hinton, \"Imagenet Classification With Deep Convolutional Neural Networks},\" in Advances in Neural InformationProcessing Systems (NIPS), pp. 1097--1105, 2012.  [3]: K. Simonyan and A. Zisserman, \"Very Deep Convolutional Networks for Large-Scale Image Recognition,\" in International Conference onLearning Representations (ICLR), 2015.",
            "title": "Refereces"
        },
        {
            "location": "/camera/picamera/",
            "text": "Using the PiCamera Module\n\n\nFor using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with:\n\n\nsudo apt-get update\n\n\nand\n\n\nsudo apt-get upgrade\n\n\nThat may take some short time.\n\n\nThen, you have to enable the camera in the raspberry settings by:\n'sudo raspi-config'\n\n\nReboot the pi after that:\n\n\nsudo reboot\n\n\nGive some time for the pi to reboot and log again into it.\n\n\nFinally, test your camera with:\n\n\nraspistill -o image.jpg\n\n\n(it takes a picture and saves it as \nimage.jpg\n)\n\n\nAfter that, a new jpeg file should appear in your directory.\n\n\nGeneral Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12)\n\n\nVisualizing pictures via ssh\n\n\nIf you want to display the images of image files in your computer, make sure to connect to the pi with:\n\n\nssh -X pi@<pi_adress>\n(focus on the \n-X\n)\n\n\nInstall \nfeh\n if you the don't have it yet:\n\n\nsudo apt-get install feh\n\n\nAnd finally do:\n\n\nfeh <image_file>.jpg",
            "title": "Picamera"
        },
        {
            "location": "/camera/picamera/#using-the-picamera-module",
            "text": "For using the PiCamera on a Raspberry Pi, you should, after connecting the camera to the Raspberry, update and upgrade your pi with:  sudo apt-get update  and  sudo apt-get upgrade  That may take some short time.  Then, you have to enable the camera in the raspberry settings by:\n'sudo raspi-config'  Reboot the pi after that:  sudo reboot  Give some time for the pi to reboot and log again into it.  Finally, test your camera with:  raspistill -o image.jpg  (it takes a picture and saves it as  image.jpg )  After that, a new jpeg file should appear in your directory.  General Helpfull information about PiCamera (https://picamera.readthedocs.io/en/release-1.12)",
            "title": "Using the PiCamera Module"
        },
        {
            "location": "/camera/picamera/#visualizing-pictures-via-ssh",
            "text": "If you want to display the images of image files in your computer, make sure to connect to the pi with:  ssh -X pi@<pi_adress> (focus on the  -X )  Install  feh  if you the don't have it yet:  sudo apt-get install feh  And finally do:  feh <image_file>.jpg",
            "title": "Visualizing pictures via ssh"
        },
        {
            "location": "/camera/webcam/",
            "text": "Webcam Video/Image\n\n\nInstall streamer:\n\n\nsudo apt-get install streamer\n\n\n\n\nRecord video or capture \n\n\nstreamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg\nstreamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi\n\n\n\n\nConvert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format)\n\n\nffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov",
            "title": "Webcam"
        },
        {
            "location": "/camera/webcam/#webcam-videoimage",
            "text": "Install streamer:  sudo apt-get install streamer  Record video or capture   streamer -q -c /dev/video0 -r 10 -t 00:00:20 -s 640x480 -o ~/test0000.jpeg\nstreamer -q -c /dev/video0 -f rgb24 -t 00:01:30 -r 10 -s 640x480 -o ~/outfile.avi  Convert to .mov to compress, then transfer. The options is set to work with Quick Time, but VLC works with any option (e.g., audio format and pixel format)  ffmpeg -i outfile.avi -acodec libmp3lame -ab 192 -pix_fmt yuv420p -r 9 output.mov",
            "title": "Webcam Video/Image"
        },
        {
            "location": "/fpga/split-DNNs/",
            "text": "FPL19 Split Networks on FPGA\n\n\nImplementation of a split, distributed CNN (ResNet V1 18), deployed to 2 \nPYNQ FPGA boards\n using \nTVM/VTA\n.\n\n\nMotivation\n\n\nImplementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs\noften require more resources than those provided by individual edge devices.\n\n\nThe idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run\nseparately on multiple edge devices in parallel.\n\n\nThe outputs from the split networks are then\nconcatenated and fed through the fully connected layers to perform inference.\n\n\nCode Description\n\n\n\n\nsplitnet.py\n contains split models built with \nMxNet Gluon\n. Only \nresnet18_v1_split\n is implemented so far.\n\n\nresnet18_v1_split\n returns a split version of \nmxnet.gluon.model_zoo.vision.resnet18_v1\n; initialized with random weights.\n\n\ndemo.py\n demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results.\n\n\nautotune.py\n uses TVM's autotuning tool to achieve fast performance when running \nresnet18_v1_split\n on PYNQ FPGA. Currently broken.\n\n\n\n\nSetup\n\n\nPYNQ Boards\n\n\nTo deploy the split networks, first acquire 2 PYNQ boards\nand set them up following instructions \nhere\n.\n\n\nAfter PYNQ boards are set up, follow instructions \nhere\n to\nlaunch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server:\n\n\nINFO:root:RPCServer: bind to 0.0.0.0:9091\n\n\n\n\nThe RPC server should be listening on port \n9091\n.\n\n\nLocal\n\n\nThe following instructions apply to your local machine. CNN models are developed, compiled\n& uploaded to PYNQ boards \nfrom your local machine\n via RPC.\n\n\nFirst, install TVM with LLVM enabled. Follow the instructions \nhere\n.\n\n\nInstall the necessary python dependencies:\n\n\npip3 install --user numpy decorator attrs\n\n\n\n\nNext, you need to add a configuration file for VTA:\n\n\ncd <tvm root>\ncp vta/config/pynq_sample.json vta/config/vta_config.json\n\n\n\n\nWhen the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to\nget the best knob parameters to achieve fast performance. Normally, for a particular network, this log file\nis generated using TVM's autotuning tool (\nautotune.py\n).\n\n\nHowever, since this tool seems to be broken, log file\nfor \nresnet18_v1_split\n was manually created.\n\n\nMove this log file to where the compiler can find it:\n\n\ncd <project root>\ncp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log\n\n\n\n\nUsage\n\n\nAfter setup has been complete on both the PYNQ and host end, you are\nnow ready to deploy the split networks. \ndemo.py\n is a minimal example that shows you how to do this.\n\n\nFirst, install additional Python dependencies:\n\n\npip3 install --user mxnet pillow\n\n\n\n\nThen run the demo:\n\n\npython3 demo.py [--cpu] [--nonsplit] [--i]\n\n\n\n\nOptions:\n\n\n\n\n\n\n--cpu\n Run model on local machine instead of PYNQ boards.\n\n\n\n\n\n\n--nonsplit\n Run the non-split version of the model.\n\n\n\n\n\n\n--i\n Run the interactive version of the demo. This allows you to enter paths to image files to feed to model.\n\n\n\n\n\n\nBy default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.",
            "title": "split DNNs"
        },
        {
            "location": "/fpga/split-DNNs/#fpl19-split-networks-on-fpga",
            "text": "Implementation of a split, distributed CNN (ResNet V1 18), deployed to 2  PYNQ FPGA boards  using  TVM/VTA .",
            "title": "FPL19 Split Networks on FPGA"
        },
        {
            "location": "/fpga/split-DNNs/#motivation",
            "text": "Implementation of deep neural networks (DNNs) are hard to achieve on edge devices because DNNs\noften require more resources than those provided by individual edge devices.  The idea of this project is to create an edge-tailored model by splitting a DNN into independent narrow DNNs to run\nseparately on multiple edge devices in parallel.  The outputs from the split networks are then\nconcatenated and fed through the fully connected layers to perform inference.",
            "title": "Motivation"
        },
        {
            "location": "/fpga/split-DNNs/#code-description",
            "text": "splitnet.py  contains split models built with  MxNet Gluon . Only  resnet18_v1_split  is implemented so far.  resnet18_v1_split  returns a split version of  mxnet.gluon.model_zoo.vision.resnet18_v1 ; initialized with random weights.  demo.py  demonstrates how to deploy split networks to 2 PYNQ FPGA boards with TVM/VTA and how to concatenate the results.  autotune.py  uses TVM's autotuning tool to achieve fast performance when running  resnet18_v1_split  on PYNQ FPGA. Currently broken.",
            "title": "Code Description"
        },
        {
            "location": "/fpga/split-DNNs/#setup",
            "text": "",
            "title": "Setup"
        },
        {
            "location": "/fpga/split-DNNs/#pynq-boards",
            "text": "To deploy the split networks, first acquire 2 PYNQ boards\nand set them up following instructions  here .  After PYNQ boards are set up, follow instructions  here  to\nlaunch TVM-based RPC servers on both boards. You should see the following output when starting the RPC server:  INFO:root:RPCServer: bind to 0.0.0.0:9091  The RPC server should be listening on port  9091 .",
            "title": "PYNQ Boards"
        },
        {
            "location": "/fpga/split-DNNs/#local",
            "text": "The following instructions apply to your local machine. CNN models are developed, compiled\n& uploaded to PYNQ boards  from your local machine  via RPC.  First, install TVM with LLVM enabled. Follow the instructions  here .  Install the necessary python dependencies:  pip3 install --user numpy decorator attrs  Next, you need to add a configuration file for VTA:  cd <tvm root>\ncp vta/config/pynq_sample.json vta/config/vta_config.json  When the TVM compiler compiles the convolutional operators in a neural network, it queries a log file to\nget the best knob parameters to achieve fast performance. Normally, for a particular network, this log file\nis generated using TVM's autotuning tool ( autotune.py ).  However, since this tool seems to be broken, log file\nfor  resnet18_v1_split  was manually created.  Move this log file to where the compiler can find it:  cd <project root>\ncp vta_v0.05.log ~/.tvm/tophub/vta_v0.05.log",
            "title": "Local"
        },
        {
            "location": "/fpga/split-DNNs/#usage",
            "text": "After setup has been complete on both the PYNQ and host end, you are\nnow ready to deploy the split networks.  demo.py  is a minimal example that shows you how to do this.  First, install additional Python dependencies:  pip3 install --user mxnet pillow  Then run the demo:  python3 demo.py [--cpu] [--nonsplit] [--i]",
            "title": "Usage"
        },
        {
            "location": "/fpga/split-DNNs/#options",
            "text": "--cpu  Run model on local machine instead of PYNQ boards.    --nonsplit  Run the non-split version of the model.    --i  Run the interactive version of the demo. This allows you to enter paths to image files to feed to model.    By default, the demo downloads 50 images of animals from Google Images, feeds them to the model, and reports the mean and standard deviation (in sec) of the inference delays.",
            "title": "Options:"
        },
        {
            "location": "/getting-started/setting-up-pi/",
            "text": "Pre-Requisite\n\n\n\n\nRaspberry Pi can ONLY be accessed from lab's network.\n\n\nConnect to \nNETGEAR79\n Wi-Fi with password \n78zBJr!4bVdpaFIQ\n.\n\n\n\n\n\n\nConnect to Raspberry Pi\n\n\nFind IP of available Pi\n\nGo to \n192.168.1.1\n in your browser with username \nadmin\n and password \npassword\n to check IP of all connected \nRaspberry Pis. Under \nconnected devices\n tab, you will be able to see the IP of all connected Raspberry Pis.\n\n\nConnect through SSH\n  \n\n\nssh pi@<ip>\n\n\n\n\nUse password \nraspberry\n for connection or for \nsudo\n command.\n\n\n\n\nPython Setup\n\n\nPython Environment\n\nWe are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING.\n\n\nPackage Installation\n\n\npip\n is always recommended for Python package installations. A cleaner way would be using \n\nvirtualenv\n, so that your environment won't interfere with others'.\n\n\nLong compling time with pip installation\n\n\nYou can use piwheels by placing the following lines in /etc/pip.conf:\n\n\n[global]\nextra-index-url=https://www.piwheels.org/simple\n\n\n\n\nThen pip will search in wheels to install any package first.\n\n\nAlso, you can download your wheel from here manually:\nhttps://pythonwheels.com/",
            "title": "Setting up pi"
        },
        {
            "location": "/getting-started/setting-up-pi/#pre-requisite",
            "text": "Raspberry Pi can ONLY be accessed from lab's network.  Connect to  NETGEAR79  Wi-Fi with password  78zBJr!4bVdpaFIQ .",
            "title": "Pre-Requisite"
        },
        {
            "location": "/getting-started/setting-up-pi/#connect-to-raspberry-pi",
            "text": "Find IP of available Pi \nGo to  192.168.1.1  in your browser with username  admin  and password  password  to check IP of all connected \nRaspberry Pis. Under  connected devices  tab, you will be able to see the IP of all connected Raspberry Pis.  Connect through SSH     ssh pi@<ip>  Use password  raspberry  for connection or for  sudo  command.",
            "title": "Connect to Raspberry Pi"
        },
        {
            "location": "/getting-started/setting-up-pi/#python-setup",
            "text": "Python Environment \nWe are using Python 2.7 for this project. PLEASE DO NOT CHANGE THE RASPBERRY PI's DEFAULT PYTHON SETTING.  Package Installation  pip  is always recommended for Python package installations. A cleaner way would be using  virtualenv , so that your environment won't interfere with others'.  Long compling time with pip installation  You can use piwheels by placing the following lines in /etc/pip.conf:  [global]\nextra-index-url=https://www.piwheels.org/simple  Then pip will search in wheels to install any package first.  Also, you can download your wheel from here manually:\nhttps://pythonwheels.com/",
            "title": "Python Setup"
        },
        {
            "location": "/getting-started/speaker-mic/",
            "text": "Speaker and Mic Setup\n\n\nAuthor: Ramyad\nDate: 7/25/2019\n\n\n\n\n\n\nList All devices: \ncat /proc/asound/cards\n\n\n\n\n\n\nList all Playback Devices \naplay -l\n\n  See your card and device number\n\n  Note: you need to set volume \nalsamixer -c <card_number>\n\n  Note: you can also use \npacmd set-source-volume <index> <volume>\n\n\n\n\n\n\nList all Recording Devices \narecord -l\n or \npacmd list-sources\n\n    See your card and device number\n\n\n\n\n\n\nControl devices with \nalsamixer\n, use F6 to select your device:wq\n\n\n\n\n\n\nSet your Recording and Playback Device as the Default PCM Devices, in \n/etc/asound.conf\n  \n\n\n\n\n\n\npcm.!default {\n  type asym\n  capture.pcm \"mic\"\n  playback.pcm \"speaker\"\n}\npcm.mic {\n  type plug\n  slave {\n    pcm \"hw:<card number>,<device number>\"\n  }\n}\npcm.speaker {\n  type plug\n  slave {\n    pcm \"hw:<card number>,<device number>\"\n  }\n}\n\n\n\n\n\n\nLive Streaming  \n\n\n\n\narecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000\n\n\n\n\n\n\n\n\nTesting Playback \nspeaker-test -t wav -c 2\n\n\n\n\n\n\nTest Recording Device  \n\n\n\n\n\n\narecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw\naplay --format=S16_LE --rate=16000 out.raw\n\n\n\n\n\n\nSpeaker and Mic Volume Control \nalsamixer -c \"card number\"",
            "title": "Speaker mic"
        },
        {
            "location": "/getting-started/speaker-mic/#speaker-and-mic-setup",
            "text": "Author: Ramyad\nDate: 7/25/2019    List All devices:  cat /proc/asound/cards    List all Playback Devices  aplay -l \n  See your card and device number \n  Note: you need to set volume  alsamixer -c <card_number> \n  Note: you can also use  pacmd set-source-volume <index> <volume>    List all Recording Devices  arecord -l  or  pacmd list-sources \n    See your card and device number    Control devices with  alsamixer , use F6 to select your device:wq    Set your Recording and Playback Device as the Default PCM Devices, in  /etc/asound.conf       pcm.!default {\n  type asym\n  capture.pcm \"mic\"\n  playback.pcm \"speaker\"\n}\npcm.mic {\n  type plug\n  slave {\n    pcm \"hw:<card number>,<device number>\"\n  }\n}\npcm.speaker {\n  type plug\n  slave {\n    pcm \"hw:<card number>,<device number>\"\n  }\n}   Live Streaming     arecord --format=S16_LE --rate=16k -D sysdefault:CARD=1 | aplay --format=S16_LE --rate=16000    Testing Playback  speaker-test -t wav -c 2    Test Recording Device      arecord --format=S16_LE --duration=5 --rate=16000 --file-type=raw out.raw\naplay --format=S16_LE --rate=16000 out.raw   Speaker and Mic Volume Control  alsamixer -c \"card number\"",
            "title": "Speaker and Mic Setup"
        },
        {
            "location": "/irobot/keyboard/",
            "text": "Control with Keyboard\n\n\nMake sure Pi is connected to power and serial  \n\n\nssh -X pi@192.168.1.2 \nsudo chmod o+rw /dev/ttyUSB0  \ngtkterm\n\n\n\n\nConfigure port to USB0\n\nChange baud rate to 115200  \n\n\npython create2_cmds.py  \n\n\n\n\nClick connect and type /dev/ttyUSB0 (the above)\n\nPress 'p' then 'f'\n\nThe robot is now controllable  \n\n\nTo see data from sensors (power):\n - View -> Hexadecimal\n\n - Log -> to somelogfile.txt\n\n - Go to python and press 'z' to begin log stream\n\n - Do whatever (ML stuff)\n\n - Stop logging from log menu when done\n\n - translatorStream.py -> change file read name to somelogfile.txt (whatever name)\n\n - python translatorStream.py\n\n - done.txt contains voltage and current values",
            "title": "Keyboard"
        },
        {
            "location": "/irobot/keyboard/#control-with-keyboard",
            "text": "Make sure Pi is connected to power and serial    ssh -X pi@192.168.1.2 \nsudo chmod o+rw /dev/ttyUSB0  \ngtkterm  Configure port to USB0 \nChange baud rate to 115200    python create2_cmds.py    Click connect and type /dev/ttyUSB0 (the above) \nPress 'p' then 'f' \nThe robot is now controllable    To see data from sensors (power):\n - View -> Hexadecimal \n - Log -> to somelogfile.txt \n - Go to python and press 'z' to begin log stream \n - Do whatever (ML stuff) \n - Stop logging from log menu when done \n - translatorStream.py -> change file read name to somelogfile.txt (whatever name) \n - python translatorStream.py \n - done.txt contains voltage and current values",
            "title": "Control with Keyboard"
        },
        {
            "location": "/irobot/navi_lidar_voice/",
            "text": "",
            "title": "Navi lidar voice"
        },
        {
            "location": "/mapping/lidar-slam/",
            "text": "Template - Lidar SLAM",
            "title": "Lidar slam"
        },
        {
            "location": "/mapping/lidar-slam/#template-lidar-slam",
            "text": "",
            "title": "Template - Lidar SLAM"
        },
        {
            "location": "/people/people/",
            "text": "Our Group\n\n\nFaculty\n\n\n* \nHyesoon Kim\n\n\nGraduate Students\n\n\n\n\nRamyad Hadidi\n\n\nJiashen Cao\n\n\n\n\n\n\nUndergraduate Students\n\n\nFall 2017\n\n\n\n\nJiashen Cao\n\n\nMatthew Woodward\n\n\n\n\nSpring 2018\n\n\n\n\nJiashen Cao\n\n\n\n\nFall 2018\n\n\n\n\nChunjun Jia\n\n\n\n\nSpring 2019\n\n\n\n\nMatthew Merck\n\n\nArthur Siqueira\n\n\nQiusen Huang\n\n\nAbhijeet Saraha\n\n\nBingyao Wang\n\n\nDongsuk Lim \n\n\nLixing Liu\n\n\nChunjun Jia\n\n\n\n\nSummer 2019\n\n\n\n\nArthur Siqueira\n\n\nAbhijeet Saraha\n\n\nChunjun Jia\n\n\nTaejoon Park\n\n\nMohan Dodda\n\n\nSayuj Shajith\n\n\nSongming Liu\n\n\nThai Tran\n\n\nJinwoo Park\n\n\nNima Shoghi\n\n\nYounmin Bae\n\n\nAkanksha Telagamsetty\n\n\nAyushi Chaudhary\n\n\nAbhi Bothera\n\n\nKabir Kohli",
            "title": "People"
        },
        {
            "location": "/people/people/#our-group",
            "text": "",
            "title": "Our Group"
        },
        {
            "location": "/people/people/#faculty",
            "text": "",
            "title": "Faculty"
        },
        {
            "location": "/people/people/#hyesoon-kim",
            "text": "",
            "title": "* Hyesoon Kim"
        },
        {
            "location": "/people/people/#graduate-students",
            "text": "Ramyad Hadidi  Jiashen Cao",
            "title": "Graduate Students"
        },
        {
            "location": "/people/people/#undergraduate-students",
            "text": "",
            "title": "Undergraduate Students"
        },
        {
            "location": "/people/people/#fall-2017",
            "text": "Jiashen Cao  Matthew Woodward",
            "title": "Fall 2017"
        },
        {
            "location": "/people/people/#spring-2018",
            "text": "Jiashen Cao",
            "title": "Spring 2018"
        },
        {
            "location": "/people/people/#fall-2018",
            "text": "Chunjun Jia",
            "title": "Fall 2018"
        },
        {
            "location": "/people/people/#spring-2019",
            "text": "Matthew Merck  Arthur Siqueira  Qiusen Huang  Abhijeet Saraha  Bingyao Wang  Dongsuk Lim   Lixing Liu  Chunjun Jia",
            "title": "Spring 2019"
        },
        {
            "location": "/people/people/#summer-2019",
            "text": "Arthur Siqueira  Abhijeet Saraha  Chunjun Jia  Taejoon Park  Mohan Dodda  Sayuj Shajith  Songming Liu  Thai Tran  Jinwoo Park  Nima Shoghi  Younmin Bae  Akanksha Telagamsetty  Ayushi Chaudhary  Abhi Bothera  Kabir Kohli",
            "title": "Summer 2019"
        },
        {
            "location": "/speech/deepspeech/",
            "text": "Deepspeech on Raspberry Pi\n\n\nRequirements: have python3 installed with pip3\n\n\nhttps://github.com/mozilla/DeepSpeech#using-the-python-package\n\n\nRun Deepspeech with Trained Model\n\n\n(use python deepspeech package) \n\n\nWARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi\n\n\nFollow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are:\n\n\nMake a virtual environment:\n\n\n\n\nPip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version\n\n\n\n\nvirtualenv -p python3 $HOME/tmp/deepspeech-venv/\n\n\n\n\n\n\nInstead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made\n\n\ndeepspeech-venv will be the name of the environment so change that if you want a different name\n\n\nOr just make a virtualenv how you normally do\n\n\n\n\nActivate the virtual environment\n\n\n\n\nNow the virtual environment is created with a bin folder with activate document\n\n\n\n\nsource $HOME/tmp/deepspeech-venv/bin/activate\n\n\n\n\nThis creates a virtual environment where you can install deepspeech related dependencies\n\n\nNow install deepspeech package on your local environment\n\n\n\n\npip3 install deepspeech\n\n\nUsing this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to)\n\n\n\n\n\n\nLinux: run this command in the directory you want to put the file: \n\nwget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz\n\n\n\n\n\n\nOthers, just enter link into web browser, this will download the file. Then manually move the file to preferred directory\n\n\n\n\n\n\nThen, unzip the file using tar command \n\ntar xvfz deepspeech-0.5.0-models.tar.gz\n\n\n\n\n\n\nThis creates a folder, called deepspeech-0.5.0-models\n\n\n\n\nNow download an audio file you want the model to do speech to text recognition\n\n\nPut this model in the preferred directory\n\n\n\n\nGo to the preferred directory on the command line and run this command:\n\ndeepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav\n\n\n\n\n\n\nEXCEPT: replace my_audio_file.wav with your audio file and \n--lm and --trie tags are optional\n\n\n\n\nReplace models with deepspeech-0.5.0-models or with the name of the folder created from the download\n\n\n\n\nMaking Your Own Model\n\n\nNext we tried to make our own model to see if we can reduce the model size:\n\n\n1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect\n\n\n\n\nYou would have to scp the newly trained model to the raspberry pi assuming trained model is small enough\n\n\n\n\n2.) If you want to use a GPU, follow directions from the gpu slack channel for conection\n\n\n\n\nUsing steps from https://github.com/mozilla/DeepSpeech#training-your-own-model:\n\n\nMake or activate your virtualenv for deepspeech\n\n\nGit clone DeepSpeech from the github \n \ngit clone https://github.com/mozilla/DeepSpeech.git\n\n\nInstall required dependencies from requirements.txt file, Run these commands\n\n\n\n\ncd deepspeech \npip3 install -r requirements.txt\n\n\n\n\n\n\nIf you are using gpu, use tensorflow gpu:\n\n\n\n\npip3 uninstall tensorflow\npip3 install 'tensorflow-gpu==1.13.1'\n\n\n\n\nDownload voice training data from common voice: https://voice.mozilla.org/en/datasets;\n\n- Download the Tatoeba dataset\n- Go to the link, scroll down to the Tatoeba dataset, press more, and press download\n- Move it to your preferrred directory\n- Unzip the file \nThe data is needs to be converted wav files.\nThe data needs to be split into train, test, and dev data\n3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript\n- Use  \nimport.py\n and \nuntilA.csv\n to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located)\n- Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder\n- Install pydub (pydub will help convert MP3 to WAV)\n\n\npip3 install pydub\n\n- (Optional) \napt-get install ffmpeg\n\n- Edit import.py before you start running the code\n- Change the fullpath variable to the directory that has the audio files\n- For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019\n- Now, run import.py by\n\n\npython3 import.py\n\n- As a result, you will have the following files:\nnew_names.csv\ntrain.csv\ndev.csv\ntest.csv\n\n\u2018new_names.csv\u2019 is just a file that contains all wav file directories\n\n- Using ./Deepspeech.py to create your own model\n\n\n./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv",
            "title": "Deepspeech"
        },
        {
            "location": "/speech/deepspeech/#deepspeech-on-raspberry-pi",
            "text": "Requirements: have python3 installed with pip3  https://github.com/mozilla/DeepSpeech#using-the-python-package",
            "title": "Deepspeech on Raspberry Pi"
        },
        {
            "location": "/speech/deepspeech/#run-deepspeech-with-trained-model",
            "text": "(use python deepspeech package)   WARNING: this model is really big: 1.6 GB; so you cannot do this on raspberry pi  Follow steps under Using Pre-trained mode on the github page (https://github.com/mozilla/DeepSpeech#using-the-python-package), using python package which are:  Make a virtual environment:   Pip3 install virtualenv if you don\u2019t have virtualenv python package yet (or pip) version   virtualenv -p python3 $HOME/tmp/deepspeech-venv/   Instead of $HOME/tmp/deepspeech-venv, put the path of where you want the virtual environment to be made  deepspeech-venv will be the name of the environment so change that if you want a different name  Or just make a virtualenv how you normally do   Activate the virtual environment   Now the virtual environment is created with a bin folder with activate document   source $HOME/tmp/deepspeech-venv/bin/activate   This creates a virtual environment where you can install deepspeech related dependencies  Now install deepspeech package on your local environment   pip3 install deepspeech  Using this: https://github.com/mozilla/DeepSpeech#getting-the-pre-trained-model, download the latest pre-trained deepspeech model: (You can use an older one if you want to)    Linux: run this command in the directory you want to put the file:  wget https://github.com/mozilla/DeepSpeech/releases/download/v0.5.0/deepspeech-0.5.0-models.tar.gz    Others, just enter link into web browser, this will download the file. Then manually move the file to preferred directory    Then, unzip the file using tar command  tar xvfz deepspeech-0.5.0-models.tar.gz    This creates a folder, called deepspeech-0.5.0-models   Now download an audio file you want the model to do speech to text recognition  Put this model in the preferred directory   Go to the preferred directory on the command line and run this command: deepspeech --model models/output_graph.pbmm --alphabet models/alphabet.txt --lm models/lm.binary --trie models/trie --audio my_audio_file.wav    EXCEPT: replace my_audio_file.wav with your audio file and \n--lm and --trie tags are optional   Replace models with deepspeech-0.5.0-models or with the name of the folder created from the download",
            "title": "Run Deepspeech with Trained Model"
        },
        {
            "location": "/speech/deepspeech/#making-your-own-model",
            "text": "Next we tried to make our own model to see if we can reduce the model size:  1.) When running on a raspberry pi, go to the \"connecting to the raspberry pi\" docs to connect   You would have to scp the newly trained model to the raspberry pi assuming trained model is small enough   2.) If you want to use a GPU, follow directions from the gpu slack channel for conection   Using steps from https://github.com/mozilla/DeepSpeech#training-your-own-model:  Make or activate your virtualenv for deepspeech  Git clone DeepSpeech from the github \n  git clone https://github.com/mozilla/DeepSpeech.git  Install required dependencies from requirements.txt file, Run these commands   cd deepspeech \npip3 install -r requirements.txt   If you are using gpu, use tensorflow gpu:   pip3 uninstall tensorflow\npip3 install 'tensorflow-gpu==1.13.1'  Download voice training data from common voice: https://voice.mozilla.org/en/datasets; \n- Download the Tatoeba dataset\n- Go to the link, scroll down to the Tatoeba dataset, press more, and press download\n- Move it to your preferrred directory\n- Unzip the file \nThe data is needs to be converted wav files.\nThe data needs to be split into train, test, and dev data\n3 csv files need to be created (for each split) which stores the wav_filename, wav_filesize, and transcript\n- Use   import.py  and  untilA.csv  to convert MP3 to WAV file while creating train.csv, dev.csv, and test.csv (The untilA.csv file tells where all the mp3 files are located)\n- Put \u2018import.py\u2019 and \u2018untilA.csv\u2019 in same folder\n- Install pydub (pydub will help convert MP3 to WAV)  pip3 install pydub \n- (Optional)  apt-get install ffmpeg \n- Edit import.py before you start running the code\n- Change the fullpath variable to the directory that has the audio files\n- For example, fullpath = \u2018/home/user/Download/tatoeba_audio_eng/tatoeba_audio_eng/audio\u2019\n- Now, run import.py by  python3 import.py \n- As a result, you will have the following files:\nnew_names.csv\ntrain.csv\ndev.csv\ntest.csv \u2018new_names.csv\u2019 is just a file that contains all wav file directories \n- Using ./Deepspeech.py to create your own model  ./DeepSpeech.py --train_files /locate/directory/here/train.csv --dev_files /locate/directory/here/dev.csv --test_files /locate/directory/here/test.csv",
            "title": "Making Your Own Model"
        },
        {
            "location": "/speech/sphinx/",
            "text": "CMU Sphinx\n\n\nAuthors: Ramyad, Sayuj\n\nDate: 7/25/2019\n\n\nFast Setup:\n\n\ngit clone https://github.com/parallel-ml/sphinxSpeech2Text\n./install.sh\nmake\n./decode\n\n\n\n\nParallel-ml repo: \nhttps://github.com/parallel-ml/sphinxSpeech2Text\n\n\nI used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions: \nlink\n\n\nThen I used the pocketsphinx_continuous command line command. There are multiple options, such as \n-inmic\n, which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the \n-infile\n flag, then type the directory of the file relative to where you are calling the command from.\n\n\nYou can change the dictionary and the language model that the program uses by using the \n-dict\n and \n-lm\n flags. I created my own dictionary an language model using a tool I found online \nlink\n, specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility.\n\n\nThe next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate.\n\n\npi@n1:/Research$ ./decode.out  \nMOVE DOWN  \nMOVE UP  \nTURN TO ME  \nTime Elapsed: 2.049368  \n\n\npi@n1:/Research$ ./decode.out  \nuh got caught  \nmove up  \nlearn to make  \nTime Elapsed: 2.049368  \n\n\n\n\nOriginally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file.\n\n\nExample of Running in Terminal\n\n\npocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm\n\n\n\n\nNote: If you get an error such as: \nerror while loading shared libraries: libpocketsphinx.so.3\n, you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below:\n\n\nexport LD_LIBRARY_PATH=/usr/local/lib\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig\n\n\n\n\nInstallation\n\n\nsudo apt-get install bison\nsudo apt-get install swig\ncd sphinxbase-5prealpha\n./autogen.sh\n.configure\nmake\nsudo make install\nexport LD_LIBRARY_PATH=/usr/local/lib\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig\ncd ../pocketsphinx-5prealpha\n./autogen.sh\n.configure\nmake\nsudo make install\n\n\n\n\nExample of Running with C\n\n\nContents of decode.c\n\n\ngcc -o decode decode.c\n\n\n\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <time.h>\n#define BILLION  1000000000.0;\n\n\nint main(void) {\n    struct timespec start, end;\n\n    system(\"export LD_LIBRARY_PATH=/usr/local/lib\");\n    system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\");\n    system(\"echo done recording...\");\n    system(\"python testfiles/noiseClean.py\");\n    system(\"echo done cleaning...\");\n    clock_gettime(CLOCK_REALTIME, &start);\n    system(\"\\\n        pocketsphinx_continuous \\\n        -infile testfiles/filtered.wav \\\n        -dict dicts/8050.dic \\\n        -lm dicts/8050.lm \\\n        2>./output/unwanted-stuff.log | tee ./output/words.txt\");\n    // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt\n    system(\"echo done decoding...\");\n    clock_gettime(CLOCK_REALTIME, &end);\n    double time_spent = (end.tv_sec - start.tv_sec) +\n            (end.tv_nsec - start.tv_nsec) / BILLION;\n    char *timerOutput = malloc(25);\n    sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent);\n    system(timerOutput);\n}\n\n\n\n\nSystem Mic Noise Fix\n\n\nUsing system/USB mic has noises, to clean, here is the content of noiseClean.py:\n\n\noutname = 'testfiles/filtered.wav'\n\ncutOffFrequency = 400.0\n\n# from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean\ndef running_mean(x, windowSize):\n  cumsum = np.cumsum(np.insert(x, 0, 0))\n  return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize\n\n# from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174\ndef interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True):\n\n    if sample_width == 1:\n        dtype = np.uint8 # unsigned char\n    elif sample_width == 2:\n        dtype = np.int16 # signed 2-byte short\n    else:\n        raise ValueError(\"Only supports 8 and 16 bit audio formats.\")\n\n    channels = np.fromstring(raw_bytes, dtype=dtype)\n\n    if interleaved:\n        # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data\n        channels.shape = (n_frames, n_channels)\n        channels = channels.T\n    else:\n        # channels are not interleaved. All samples from channel M occur before all samples from channel M-1\n        channels.shape = (n_channels, n_frames)\n\n    return channels\n\nwith contextlib.closing(wave.open(fname,'rb')) as spf:\n    sampleRate = spf.getframerate()\n    ampWidth = spf.getsampwidth()\n    nChannels = spf.getnchannels()\n    nFrames = spf.getnframes()\n\n    # Extract Raw Audio from multi-channel Wav File\n    signal = spf.readframes(nFrames*nChannels)\n    spf.close()\n    channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True)\n\n    # get window size\n    # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter\n    freqRatio = (cutOffFrequency/sampleRate)\n    N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio)\n\n    # Use moviung average (only on first channel)\n    filtered = running_mean(channels[0], N).astype(channels.dtype)\n\n    wav_file = wave.open(outname, \"w\")\n    wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname()))\n    wav_file.writeframes(filtered.tobytes('C'))\n    wav_file.close()",
            "title": "Sphinx"
        },
        {
            "location": "/speech/sphinx/#cmu-sphinx",
            "text": "Authors: Ramyad, Sayuj \nDate: 7/25/2019  Fast Setup:  git clone https://github.com/parallel-ml/sphinxSpeech2Text\n./install.sh\nmake\n./decode  Parallel-ml repo:  https://github.com/parallel-ml/sphinxSpeech2Text  I used the pocketsphinx to decode the audio files on the raspberry pi\u2019s. I installed it on the raspberry pi by following these instructions:  link  Then I used the pocketsphinx_continuous command line command. There are multiple options, such as  -inmic , which while use the system\u2019s default microphone to detect and live decode the speech. You can also decode files using the  -infile  flag, then type the directory of the file relative to where you are calling the command from.  You can change the dictionary and the language model that the program uses by using the  -dict  and  -lm  flags. I created my own dictionary an language model using a tool I found online  link , specifically made for pocketsphinx. I did this so that we could reduce the language model size to improve performance and accuracy. I found that the performance was 6x faster when I used my reduced dictionary, and obviously the accuracy is better, but it loses flexibility.  The next steps are to increase the dictionary to include a more variety of words, and increase the flexibility of commands that can be given to the raspberry pi. Below I have attached pictures of terminal output that shows the difference in performance. The output on the top shows performance with smaller dictionary and language model, the output on the bottom is the original dictionary that pocketsphinx comes with. It took more than 6x longer and it was less accurate.  pi@n1:/Research$ ./decode.out  \nMOVE DOWN  \nMOVE UP  \nTURN TO ME  \nTime Elapsed: 2.049368  \n\n\npi@n1:/Research$ ./decode.out  \nuh got caught  \nmove up  \nlearn to make  \nTime Elapsed: 2.049368    Originally it verbosely outputs every step while it processes the audio, and it was hard to find the actual output, so I created a command to output all the unwanted logs to a specific file, and the actual decoded speech into it\u2019s own file.",
            "title": "CMU Sphinx"
        },
        {
            "location": "/speech/sphinx/#example-of-running-in-terminal",
            "text": "pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm  Note: If you get an error such as:  error while loading shared libraries: libpocketsphinx.so.3 , you may want to check your linker configuration of the LD_LIBRARY_PATH environment variable described below:  export LD_LIBRARY_PATH=/usr/local/lib\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig",
            "title": "Example of Running in Terminal"
        },
        {
            "location": "/speech/sphinx/#installation",
            "text": "sudo apt-get install bison\nsudo apt-get install swig\ncd sphinxbase-5prealpha\n./autogen.sh\n.configure\nmake\nsudo make install\nexport LD_LIBRARY_PATH=/usr/local/lib\nexport PKG_CONFIG_PATH=/usr/local/lib/pkgconfig\ncd ../pocketsphinx-5prealpha\n./autogen.sh\n.configure\nmake\nsudo make install",
            "title": "Installation"
        },
        {
            "location": "/speech/sphinx/#example-of-running-with-c",
            "text": "Contents of decode.c  gcc -o decode decode.c  #include <stdlib.h>\n#include <stdio.h>\n#include <time.h>\n#define BILLION  1000000000.0;\n\n\nint main(void) {\n    struct timespec start, end;\n\n    system(\"export LD_LIBRARY_PATH=/usr/local/lib\");\n    system(\"arecord --format=S16_LE --duration=5 --rate=16k -D sysdefault:CARD=1 --file-type=wav testfiles/noisy.wav\");\n    system(\"echo done recording...\");\n    system(\"python testfiles/noiseClean.py\");\n    system(\"echo done cleaning...\");\n    clock_gettime(CLOCK_REALTIME, &start);\n    system(\"\\\n        pocketsphinx_continuous \\\n        -infile testfiles/filtered.wav \\\n        -dict dicts/8050.dic \\\n        -lm dicts/8050.lm \\\n        2>./output/unwanted-stuff.log | tee ./output/words.txt\");\n    // pocketsphinx_continuous -infile testfiles/Untitled.wav -dict dicts/8050.dic -lm dicts/8050.lm 2>./output/unwanted-stuff.log | tee ./output/words.txt\n    system(\"echo done decoding...\");\n    clock_gettime(CLOCK_REALTIME, &end);\n    double time_spent = (end.tv_sec - start.tv_sec) +\n            (end.tv_nsec - start.tv_nsec) / BILLION;\n    char *timerOutput = malloc(25);\n    sprintf(timerOutput, \"echo Time Elapsed: %f\\n\", time_spent);\n    system(timerOutput);\n}",
            "title": "Example of Running with C"
        },
        {
            "location": "/speech/sphinx/#system-mic-noise-fix",
            "text": "Using system/USB mic has noises, to clean, here is the content of noiseClean.py:  outname = 'testfiles/filtered.wav'\n\ncutOffFrequency = 400.0\n\n# from http://stackoverflow.com/questions/13728392/moving-average-or-running-mean\ndef running_mean(x, windowSize):\n  cumsum = np.cumsum(np.insert(x, 0, 0))\n  return (cumsum[windowSize:] - cumsum[:-windowSize]) / windowSize\n\n# from http://stackoverflow.com/questions/2226853/interpreting-wav-data/2227174#2227174\ndef interpret_wav(raw_bytes, n_frames, n_channels, sample_width, interleaved = True):\n\n    if sample_width == 1:\n        dtype = np.uint8 # unsigned char\n    elif sample_width == 2:\n        dtype = np.int16 # signed 2-byte short\n    else:\n        raise ValueError(\"Only supports 8 and 16 bit audio formats.\")\n\n    channels = np.fromstring(raw_bytes, dtype=dtype)\n\n    if interleaved:\n        # channels are interleaved, i.e. sample N of channel M follows sample N of channel M-1 in raw data\n        channels.shape = (n_frames, n_channels)\n        channels = channels.T\n    else:\n        # channels are not interleaved. All samples from channel M occur before all samples from channel M-1\n        channels.shape = (n_channels, n_frames)\n\n    return channels\n\nwith contextlib.closing(wave.open(fname,'rb')) as spf:\n    sampleRate = spf.getframerate()\n    ampWidth = spf.getsampwidth()\n    nChannels = spf.getnchannels()\n    nFrames = spf.getnframes()\n\n    # Extract Raw Audio from multi-channel Wav File\n    signal = spf.readframes(nFrames*nChannels)\n    spf.close()\n    channels = interpret_wav(signal, nFrames, nChannels, ampWidth, True)\n\n    # get window size\n    # from http://dsp.stackexchange.com/questions/9966/what-is-the-cut-off-frequency-of-a-moving-average-filter\n    freqRatio = (cutOffFrequency/sampleRate)\n    N = int(math.sqrt(0.196196 + freqRatio**2)/freqRatio)\n\n    # Use moviung average (only on first channel)\n    filtered = running_mean(channels[0], N).astype(channels.dtype)\n\n    wav_file = wave.open(outname, \"w\")\n    wav_file.setparams((1, ampWidth, sampleRate, nFrames, spf.getcomptype(), spf.getcompname()))\n    wav_file.writeframes(filtered.tobytes('C'))\n    wav_file.close()",
            "title": "System Mic Noise Fix"
        },
        {
            "location": "/speech/text-to-speech/",
            "text": "Template - Text to Speech",
            "title": "Text to speech"
        },
        {
            "location": "/speech/text-to-speech/#template-text-to-speech",
            "text": "",
            "title": "Template - Text to Speech"
        },
        {
            "location": "/vision/character/",
            "text": "Character Recognition",
            "title": "Character"
        },
        {
            "location": "/vision/character/#character-recognition",
            "text": "",
            "title": "Character Recognition"
        }
    ]
}